- **스미더리 크롤링 프로젝트 그리고 개요**
    
    서울경제신문 AI TF팀은 기자들의 업무 효율성을 향상시키기 위해 MCP(Model Context Protocol) 서버 크롤링 프로젝트를 진행하였다. 본 프로젝트는 기자들이 뉴스 데이터를 더 효과적으로 분석하고 처리할 수 있는 AI 도구를 발굴하는 것을 목적으로 하였다. MCP 서버는 AI 모델과 상호작용하여 특화된 기능을 제공하는 도구로, 이를 활용하면 기사 작성, 데이터 분석, 이슈 트래킹 등 기자들의 다양한 업무에 도움을 줄 수 있다.
    
    스미더리(Smithery) 사이트에 등록된 약 4,000개의 MCP 서버 정보를 수집하기 위해 자동화 크롤링 시스템을 구축하였다. 이 시스템은 Python 언어를 기반으로 BeautifulSoup(HTML 파싱 라이브러리)와 requests(HTTP 요청 라이브러리)를 활용하여 개발되었다. 수작업으로 진행했다면 수 주가 소요될 작업을 자동화함으로써 데이터 수집 시간을 약 99% 단축하였고, 사람의 실수 없이 정확한 데이터를 확보할 수 있었다.
    
    SmitheryMultiQueryCrawler 클래스를 중심으로 구현된 이 시스템은 다중 검색어 지원, 페이지 자동 감지, 유연한 HTML 파싱, 서버 상세 정보 추출 등의 핵심 기능을 갖추었다. 크롤링 과정은 검색어 설정, 페이지 로드, 서버 카드 추출, 서버 데이터 파싱, CSV 파일 저장으로 이어지는 순차적 흐름으로 진행되었다. 특히 시스템에 랜덤 대기 시간과 예외 처리 로직을 구현하여 안정적인 크롤링을 보장하였으며, 웹사이트 구조 변화에도 강인하게 대응할 수 있게 설계하였다.
    
    이 프로젝트를 통해 수집된 MCP 서버 데이터는 서울경제신문의 뉴스 데이터 분석과 AI 도구 개발에 중요한 기초 자료로 활용될 것이다. 특히 국제부를 위한 외신 브리핑 센터, AI 기사 초안 도우미, 글로벌 이슈 맥락 분석기 등의 도구 개발에 직접적으로 기여할 것이다. 서울경제신문의 65년 축적 데이터와 빅카인즈의 1억 건 기사 데이터를 AI와 MCP를 통해 맥락화하여 이슈 중심의 정보를 제공하는 것이 최종 목표이다.
    
    10월 말까지 프로젝트 완료를 목표로 하고 있으며, 향후 MCP 서버를 활용한 뉴스 데이터 맥락화 및 이슈 중심 분석 시스템 구축을 통해 개별 뉴스가 아닌 이슈 흐름을 파악할 수 있는 지능형 뉴스룸 환경 조성에 기여할 것이다. 궁극적으로 이 프로젝트는 기자들의 업무 시간을 30% 이상 절약하고, 더 깊이 있는 취재와 분석에 집중할 수 있는 환경을 만드는 데 핵심적인 역할을 할 것으로 기대된다.